{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 標準使用ライブラリー\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from icecream import ic\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# 追記\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# debug\n",
    "#%pdb on\n",
    "\n",
    "import pixiedust #%pixie_debugger\n",
    "\n",
    "# tfがエラーはかないため\n",
    "# tfがエラーはかないため\n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prof (topic_idx, top_n = 100):\n",
    "    \n",
    "    topics = lda.transform(X)\n",
    "    prof_idx_list = topics[:, topic_idx].argsort()[:-top_n - 1:-1]\n",
    "    return [docs[d] for d in prof_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "df = pd.read_csv(\"../data/result0605.csv\", engine='python')\n",
    "\n",
    "type(df[\"description\"])\n",
    "docs = df[\"description\"].to_list()\n",
    "\n",
    "\n",
    "import subprocess\n",
    "cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                           shell=True).communicate()[0]).decode('utf-8')\n",
    "import MeCab\n",
    "m=MeCab.Tagger(\"-Ochasen -d \"+str(path_neologd))\n",
    "\n",
    "\n",
    "def neolog_prep_text( text):\n",
    "  return_words = []\n",
    "  splited_text = (re.split('[\\t,]', line) for line in m.parse(text).split('\\n'))\n",
    "  for tmp_word in splited_text :\n",
    "    if (tmp_word[0] in ('EOS', '', 't', 'ー') ):\n",
    "       continue \n",
    "    if not re.match( '名詞' ,tmp_word[3]  ) or tmp_word[0] in emoji.UNICODE_EMOJI[\"en\"]:\n",
    "      continue\n",
    "    else:\n",
    "      return_words.append(tmp_word[0])\n",
    "    \n",
    "    \n",
    "    # 1回しかでてこないのを削る。\n",
    "\n",
    "  return return_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93794/93794 [01:07<00:00, 1387.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "new_docs = list()\n",
    "for doc in tqdm(docs):\n",
    "  if str(doc) == \"nan\":\n",
    "    continue\n",
    "  tmp_words =  neolog_prep_text(str(doc))\n",
    "  new_docs.append( tmp_words )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(doc) for doc in new_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\"):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "          stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "path = \"stop_words.txt\"\n",
    "download_stopwords(path)\n",
    "stop_words = create_stopwords(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=30, stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../0701expt/0703_1114/0703_1156_lda.pickle', 'rb') as f:\n",
    "    lda = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=1500, doc_topic_prior=0.5,\n",
       "                          evaluate_every=1, learning_decay=1.3,\n",
       "                          learning_method='online', learning_offset=50,\n",
       "                          max_doc_update_iter=100, max_iter=50,\n",
       "                          mean_change_tol=0.0001, n_components=8, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=1, topic_word_prior=0.15,\n",
       "                          total_samples=1000000.0, verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88481"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
