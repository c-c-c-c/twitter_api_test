{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/11_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 標準使用ライブラリー\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from icecream import ic\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "\n",
    "\n",
    "# 追記\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# debug\n",
    "#%pdb on\n",
    "\n",
    "import pixiedust #%pixie_debugger\n",
    "\n",
    "# tfがエラーはかないため\n",
    "# tfがエラーはかないため\n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "df = pd.read_csv(\"../data/result0605.csv\", engine='python')\n",
    "\n",
    "type(df[\"description\"])\n",
    "docs = df[\"description\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RhY0Iq0ycIv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93794\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM4WRXtQgL9G"
   },
   "source": [
    "### Neologdを使ってtokenizeする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1LVnUt6FgYbW"
   },
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import MeCab\n",
    "\n",
    "def make_neologd_tagger():\n",
    "    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                               shell=True).communicate()[0]).decode('utf-8')\n",
    "    m=MeCab.Tagger(\"-Ochasen -d \"+str(path_neologd))\n",
    "    return (m)\n",
    "\n",
    "\n",
    "def neolog_prep_text( text, m):\n",
    "    return_words = []\n",
    "\n",
    "    \n",
    "    splited_text = (re.split('[\\t,]', line) for line in m.parse(text).split('\\n'))\n",
    "    for tmp_word in splited_text :\n",
    "        if (tmp_word[0] in ('EOS', '', 't', 'ー') ):\n",
    "           continue \n",
    "        if not re.match( '名詞' ,tmp_word[3]  ) or tmp_word[0] in emoji.UNICODE_EMOJI[\"en\"]:\n",
    "            continue\n",
    "        else:\n",
    "            return_words.append(tmp_word[0])\n",
    "\n",
    "    return return_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPvZBojxgYcW"
   },
   "source": [
    "* tokenizationの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1iICBt6EgqC-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93794/93794 [00:39<00:00, 2345.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "m = make_neologd_tagger()\n",
    "\n",
    "new_docs = list()\n",
    "for doc in tqdm(docs):\n",
    "  if str(doc) == \"nan\":\n",
    "    continue\n",
    "  tmp_words =  neolog_prep_text(str(doc), m)\n",
    "  new_docs.append( tmp_words )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBDnVSoZgaMJ"
   },
   "source": [
    "* tokenizationの結果を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p6MVn-YGhBst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['過去', 'ジャパリカート', '動画', 'TSUMURI', 'KART', 'VRChat', 'ワリスノ', 'MK', '8', 'DX', '一位', 'りし', 'た人', '社会', '出て', '配信', 'https', 'co', 'FJoitl', '8', 'JHE', 'ヘッダ', '猫', '飼い主', 'smmmmm']\n"
     ]
    }
   ],
   "source": [
    "print(new_docs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7lABQFJgdBj"
   },
   "source": [
    "* 各文書を長い文字列で表しなおす（CountVectorizerを後で使うため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mnNRbzLzi6vh"
   },
   "outputs": [],
   "source": [
    "corpus = [' '.join(doc) for doc in new_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nu0eT7OXCzD"
   },
   "source": [
    "## 11-02 データ行列の作成\n",
    "* LDAの場合、単に単語の出現頻度を重みとして各文書をベクトル化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo8oPlaMCzoM"
   },
   "source": [
    "### sklearnのCountVectorizerで疎行列化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pyLBUfSXTUL"
   },
   "source": [
    "* 全文書の半分より多い文書に現れる単語は、高頻度語とみなして削除する。\n",
    "* 30件未満の文書にしか現れない単語は、低頻度語とみなして削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\"):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "          stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "path = \"stop_words.txt\"\n",
    "download_stopwords(path)\n",
    "stop_words = create_stopwords(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zrj0mmMrCzNI"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MIN_DF = 30\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df= MIN_DF, stop_words=stop_words)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XoE0bBHJEFEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2wVko9vXnlq"
   },
   "source": [
    "* 文書数と語彙サイズを変数にセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kAd821DGFXXp"
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8puM9OXrNk"
   },
   "source": [
    "### TF-IDFで各文書における単語の重みを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ywX2HtW-Elar"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "Xtfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hdac5_tSE4YC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4227)\t0.603862693464167\n",
      "  (0, 4148)\t0.4706802938434637\n",
      "  (0, 3490)\t0.1847556021921528\n",
      "  (0, 3414)\t0.33418592758883475\n",
      "  (0, 3402)\t0.3582132311126842\n",
      "  (0, 1747)\t0.24047370992039763\n",
      "  (0, 1109)\t0.28609564411708244\n"
     ]
    }
   ],
   "source": [
    "print(Xtfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CY0mRZonFEJF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88481, 5100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXUdhYDMYuDO"
   },
   "source": [
    "### LDAのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5L34qQ1iFncJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPHsNjupYp7w"
   },
   "source": [
    "### トピックの重要語を取り出す関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rpoC-pofHMEO"
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words=30):\n",
    "  top_features = list()\n",
    "  weights = list()\n",
    "  for topic_idx, topic in enumerate(model.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features.append([feature_names[i] for i in top_features_ind])\n",
    "    weights.append(topic[top_features_ind])\n",
    "  return top_features, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EHfuc6RZgPh"
   },
   "source": [
    "# LDAでトピック抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSAEThMwZjhb"
   },
   "source": [
    "### LDAによるトピック抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NFf5jcX5b45d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_word_cloud(n_components, lda):\n",
    "    # matplotlib and seaborn for plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    plt.style.use('dark_background')\n",
    "    top_words, weights = get_top_words(lda, vectorizer.get_feature_names())\n",
    "    topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]\n",
    "    FONT_PATH = \"/usr/share/fonts/opentype/ipaexfont-mincho/ipaexm.ttf\"\n",
    "    cloud = WordCloud(stopwords=STOPWORDS,\n",
    "                  font_path=FONT_PATH,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=100,\n",
    "                  colormap='tab10'\n",
    "                  )\n",
    "\n",
    "    tate = math.ceil(len(topic_words) / 2)\n",
    "    fig, axes = plt.subplots(tate, 2, figsize=(32, 50), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "        if i >= len(topic_words):\n",
    "            break\n",
    "\n",
    "        fig.add_subplot(ax)\n",
    "        cloud.generate_from_frequencies(topic_words[i], max_font_size=500)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    pdf = PdfPages( \n",
    "        (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M') + 'topic.pdf')\n",
    "\n",
    "\n",
    "    fignums = plt.get_fignums()\n",
    "    for fignum in fignums:\n",
    "        plt.figure(fignum)\n",
    "        pdf.savefig()\n",
    "\n",
    "    pdf.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KuuSTK6fZfu7"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "def lda_main (now_, batch_size ,n_components, topic_word_prior,doc_topic_prior  ,max_iter=30):\n",
    "\n",
    "    folder_name = now_\n",
    "\n",
    "    # フォルダを作成\n",
    "    os.mkdir(\"../0714expt/\"+folder_name)\n",
    "    os.chdir(\"../0714expt/\"+folder_name)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                    max_iter=max_iter,\n",
    "                                    topic_word_prior=topic_word_prior, # トピック数の逆数が目安の0.01,0.02,0.05,0.1などなど試す\n",
    "                                    doc_topic_prior =  doc_topic_prior, \n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50,\n",
    "                                    batch_size= batch_size,# 多くする\n",
    "                                    learning_decay = 0.7,\n",
    "                                    mean_change_tol=1e-4,\n",
    "                                    random_state=1,\n",
    "                                    evaluate_every=1,\n",
    "                                    verbose=1)\n",
    "    print((f\"Fitting LDA models with tf features, \"\n",
    "    f\"n_samples={n_samples} and n_features={n_features}\"))\n",
    "    t0 = time()\n",
    "    lda.fit(X)\n",
    "    print(f\"done in {time() - t0:0.3f}s.\")\n",
    "    # パラメータの比較はperplexity\n",
    "    # ハイパーパラメータ調整を頑張る！（やってられない！といわない！！）\n",
    "    \n",
    "    \n",
    "    coherance = metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=20, # これはデフォルトが20\n",
    "                        topic_word_distrib=lda.components_, \n",
    "                        dtm=Xtfidf,  # tfidfの結果\n",
    "                        vocab=np.array([x for x in vectorizer.vocabulary_.keys()]), \n",
    "                        texts=new_docs)\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "            \"perplexity\" : lda.perplexity(X) ,\n",
    "            \"coherance\": coherance,\n",
    "        }\n",
    "\n",
    "    logger.warning('TIME:{0}'.format(now_) )\n",
    "    logger.warning('MIN_DF:{0}'.format(MIN_DF) )\n",
    "    logger.warning('params:batch_size:{0}'.format(batch_size)) \n",
    "    logger.warning('params:n_components:{0}'.format(n_components)) \n",
    "    logger.warning('params:topic_word_prior:{0}'.format(topic_word_prior)) \n",
    "    logger.warning('params:doc_topic_prior:{0}'.format(doc_topic_prior)) \n",
    "    logger.warning('params:max_iter:{0}'.format(max_iter)) \n",
    "    logger.warning('done n_iter:{0}'.format(lda.n_iter_)) \n",
    "    logger.warning('perplexity:{0}'.format(results[\"perplexity\"])) \n",
    "    logger.warning('coherance:{0}'.format(results[\"coherance\"]) )\n",
    "    logger.warning('check all params:{0}'.format(lda.get_params() )) \n",
    "    make_word_cloud(n_components, lda)\n",
    "    # pickle\n",
    "    file_name = now_ + '_lda.pickle'\n",
    "    with open(file_name, mode=\"wb\") as f:\n",
    "        pickle.dump(lda, f)\n",
    "    \n",
    "#     breakpoint()\n",
    "    \n",
    "    os.chdir(\"../../\")\n",
    "    return(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qWd3DND8aAVz"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../0714expt/0714_2228_07'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-44f7a9a14c44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnow_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%m%d_%H%M_%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlda_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_word_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_word_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_topic_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-210f9070edd8>\u001b[0m in \u001b[0;36mlda_main\u001b[0;34m(now_, batch_size, n_components, topic_word_prior, doc_topic_prior, max_iter)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# フォルダを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../0714expt/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../0714expt/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../0714expt/0714_2228_07'"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../experiment/0714expt/mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "for batch_size,n_components, topic_word_prior, doc_topic_prior in itertools.product([1500] ,[6, 8, 10,12,15,18,22,25,30,35,40], # 6,8 のあと落ちた、、\n",
    "                                                                                    [0.005,0.01, 0.03,0.05, 0.15, 0.3, 0.4, 0.5, 0.6, 0.8],[0.005,0.01, 0.03,0.05, 0.15, 0.3, 0.4, 0.5, 0.6, 0.8] ):\n",
    "    now_ = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')\n",
    "    \n",
    "    lda_main(now_, n_components=n_components, topic_word_prior=topic_word_prior, doc_topic_prior = doc_topic_prior, batch_size=batch_size, max_iter=50)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pez5Ak2-cUjS"
   },
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_HRjNZJcbNr"
   },
   "source": [
    "### プロフィールを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prof (topic_idx, top_n = 100):\n",
    "    \n",
    "    topics = lda.transform(X)\n",
    "    prof_idx_list = topics[:, topic_idx].argsort()[:-top_n - 1:-1]\n",
    "    return [docs[d] for d in prof_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_top_prof(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nownow_file = (datetime.datetime.now() + datetime.timedelta(hours=9) ).strftime('%m%d_%H%M')+\"topic_modeling.ipynb\"\n",
    "\n",
    "!cp ./topic_modeling.ipynb ./jupyter_backup_for_param/$nownow_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')+\"_output.txt\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyVTNTZxoqrFDwo2+taBP8",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "12MxXsGfePft6pAHKkHrg23Yb7t_RSKiN",
   "name": "11_topic_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
