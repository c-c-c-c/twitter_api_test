{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/11_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 標準使用ライブラリー\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from icecream import ic\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "\n",
    "\n",
    "# 追記\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# debug\n",
    "#%pdb on\n",
    "\n",
    "import pixiedust #%pixie_debugger\n",
    "\n",
    "# tfがエラーはかないため\n",
    "# tfがエラーはかないため\n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "df = pd.read_csv(\"../data/result0605.csv\", engine='python')\n",
    "\n",
    "type(df[\"description\"])\n",
    "docs = df[\"description\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RhY0Iq0ycIv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93794\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM4WRXtQgL9G"
   },
   "source": [
    "### Neologdを使ってtokenizeする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1LVnUt6FgYbW"
   },
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import MeCab\n",
    "\n",
    "def make_neologd_tagger():\n",
    "    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                               shell=True).communicate()[0]).decode('utf-8')\n",
    "    m=MeCab.Tagger(\"-Ochasen -d \"+str(path_neologd))\n",
    "    return (m)\n",
    "\n",
    "\n",
    "def neolog_prep_text( text, m):\n",
    "    return_words = []\n",
    "\n",
    "    \n",
    "    splited_text = (re.split('[\\t,]', line) for line in m.parse(text).split('\\n'))\n",
    "    for tmp_word in splited_text :\n",
    "        if (tmp_word[0] in ('EOS', '', 't', 'ー') ):\n",
    "           continue \n",
    "        if not re.match( '名詞' ,tmp_word[3]  ) or tmp_word[0] in emoji.UNICODE_EMOJI[\"en\"]:\n",
    "            continue\n",
    "        else:\n",
    "            return_words.append(tmp_word[0])\n",
    "\n",
    "    return return_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPvZBojxgYcW"
   },
   "source": [
    "* tokenizationの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1iICBt6EgqC-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93794/93794 [00:24<00:00, 3825.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "m = make_neologd_tagger()\n",
    "\n",
    "new_docs = list()\n",
    "for doc in tqdm(docs):\n",
    "  if str(doc) == \"nan\":\n",
    "    continue\n",
    "  tmp_words =  neolog_prep_text(str(doc), m)\n",
    "  new_docs.append( tmp_words )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBDnVSoZgaMJ"
   },
   "source": [
    "* tokenizationの結果を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p6MVn-YGhBst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['過去', 'ジャパリカート', '動画', 'TSUMURI', 'KART', 'VRChat', 'ワリスノ', 'MK', '8', 'DX', '一位', 'りし', 'た人', '社会', '出て', '配信', 'https', 'co', 'FJoitl', '8', 'JHE', 'ヘッダ', '猫', '飼い主', 'smmmmm']\n"
     ]
    }
   ],
   "source": [
    "print(new_docs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7lABQFJgdBj"
   },
   "source": [
    "* 各文書を長い文字列で表しなおす（CountVectorizerを後で使うため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mnNRbzLzi6vh"
   },
   "outputs": [],
   "source": [
    "corpus = [' '.join(doc) for doc in new_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nu0eT7OXCzD"
   },
   "source": [
    "## 11-02 データ行列の作成\n",
    "* LDAの場合、単に単語の出現頻度を重みとして各文書をベクトル化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo8oPlaMCzoM"
   },
   "source": [
    "### sklearnのCountVectorizerで疎行列化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pyLBUfSXTUL"
   },
   "source": [
    "* 全文書の半分より多い文書に現れる単語は、高頻度語とみなして削除する。\n",
    "* 30件未満の文書にしか現れない単語は、低頻度語とみなして削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\"):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "          stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "path = \"stop_words.txt\"\n",
    "download_stopwords(path)\n",
    "stop_words = create_stopwords(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zrj0mmMrCzNI"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MIN_DF = 30\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df= MIN_DF, stop_words=stop_words)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XoE0bBHJEFEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2wVko9vXnlq"
   },
   "source": [
    "* 文書数と語彙サイズを変数にセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kAd821DGFXXp"
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8puM9OXrNk"
   },
   "source": [
    "### TF-IDFで各文書における単語の重みを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ywX2HtW-Elar"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "Xtfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hdac5_tSE4YC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4227)\t0.603862693464167\n",
      "  (0, 4148)\t0.4706802938434637\n",
      "  (0, 3490)\t0.1847556021921528\n",
      "  (0, 3414)\t0.33418592758883475\n",
      "  (0, 3402)\t0.3582132311126842\n",
      "  (0, 1747)\t0.24047370992039763\n",
      "  (0, 1109)\t0.28609564411708244\n"
     ]
    }
   ],
   "source": [
    "print(Xtfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CY0mRZonFEJF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88481, 5100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXUdhYDMYuDO"
   },
   "source": [
    "### LDAのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5L34qQ1iFncJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPHsNjupYp7w"
   },
   "source": [
    "### トピックの重要語を取り出す関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rpoC-pofHMEO"
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words=30):\n",
    "  top_features = list()\n",
    "  weights = list()\n",
    "  for topic_idx, topic in enumerate(model.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features.append([feature_names[i] for i in top_features_ind])\n",
    "    weights.append(topic[top_features_ind])\n",
    "  return top_features, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EHfuc6RZgPh"
   },
   "source": [
    "# LDAでトピック抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSAEThMwZjhb"
   },
   "source": [
    "### LDAによるトピック抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NFf5jcX5b45d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_word_cloud(n_components, lda):\n",
    "    # matplotlib and seaborn for plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    plt.style.use('dark_background')\n",
    "    top_words, weights = get_top_words(lda, vectorizer.get_feature_names())\n",
    "    topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]\n",
    "    FONT_PATH = \"/usr/share/fonts/opentype/ipaexfont-mincho/ipaexm.ttf\"\n",
    "    cloud = WordCloud(stopwords=STOPWORDS,\n",
    "                  font_path=FONT_PATH,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=100,\n",
    "                  colormap='tab10'\n",
    "                  )\n",
    "\n",
    "    tate = math.ceil(len(topic_words) / 2)\n",
    "    fig, axes = plt.subplots(tate, 2, figsize=(32, 50), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "        if i >= len(topic_words):\n",
    "            break\n",
    "\n",
    "        fig.add_subplot(ax)\n",
    "        cloud.generate_from_frequencies(topic_words[i], max_font_size=500)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    pdf = PdfPages( \n",
    "                    now_ +'topic.pdf')\n",
    "\n",
    "    fignums = plt.get_fignums()\n",
    "    for fignum in fignums:\n",
    "        plt.figure(fignum)\n",
    "        pdf.savefig()\n",
    "\n",
    "    pdf.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KuuSTK6fZfu7"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "def lda_main (now_, batch_size ,n_components, topic_word_prior,doc_topic_prior  ,max_iter=30):\n",
    "\n",
    "    folder_name = now_\n",
    "\n",
    "    # フォルダを作成\n",
    "    os.mkdir(\"../experiment/0714expt/\"+folder_name)\n",
    "    os.chdir(\"../experiment/0714expt/\"+folder_name)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                    max_iter=max_iter,\n",
    "                                    topic_word_prior=topic_word_prior, # トピック数の逆数が目安の0.01,0.02,0.05,0.1などなど試す\n",
    "                                    doc_topic_prior =  doc_topic_prior, \n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50,\n",
    "                                    batch_size= batch_size,# 多くする\n",
    "                                    learning_decay = 0.7,\n",
    "                                    mean_change_tol=1e-4,\n",
    "                                    random_state=1,\n",
    "                                    evaluate_every=1,\n",
    "                                    verbose=1)\n",
    "    print((f\"Fitting LDA models with tf features, \"\n",
    "    f\"n_samples={n_samples} and n_features={n_features}\"))\n",
    "    t0 = time()\n",
    "    lda.fit(X)\n",
    "    print(f\"done in {time() - t0:0.3f}s.\")\n",
    "    # パラメータの比較はperplexity\n",
    "    # ハイパーパラメータ調整を頑張る！（やってられない！といわない！！）\n",
    "    \n",
    "    \n",
    "    coherance = metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=20, # これはデフォルトが20\n",
    "                        topic_word_distrib=lda.components_, \n",
    "                        dtm=Xtfidf,  # tfidfの結果\n",
    "                        vocab=np.array([x for x in vectorizer.vocabulary_.keys()]), \n",
    "                        texts=new_docs)\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "            \"perplexity\" : lda.perplexity(X) ,\n",
    "            \"coherance\": coherance,\n",
    "        }\n",
    "\n",
    "    logger.warning('TIME:{0}'.format(now_) )\n",
    "    logger.warning('MIN_DF:{0}'.format(MIN_DF) )\n",
    "    logger.warning('params:batch_size:{0}'.format(batch_size)) \n",
    "    logger.warning('params:n_components:{0}'.format(n_components)) \n",
    "    logger.warning('params:topic_word_prior:{0}'.format(topic_word_prior)) \n",
    "    logger.warning('params:doc_topic_prior:{0}'.format(doc_topic_prior)) \n",
    "    logger.warning('params:max_iter:{0}'.format(max_iter)) \n",
    "    logger.warning('done n_iter:{0}'.format(lda.n_iter_)) \n",
    "    logger.warning('perplexity:{0}'.format(results[\"perplexity\"])) \n",
    "    logger.warning('coherance:{0}'.format(results[\"coherance\"]) )\n",
    "    logger.warning('check all params:{0}'.format(lda.get_params() )) \n",
    "    make_word_cloud(n_components, lda)\n",
    "    # pickle\n",
    "    file_name = now_ + '_lda.pickle'\n",
    "    with open(file_name, mode=\"wb\") as f:\n",
    "        pickle.dump(lda, f)\n",
    "    \n",
    "#     breakpoint()\n",
    "    \n",
    "    os.chdir(\"../../\")\n",
    "    return(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWd3DND8aAVz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2037.8422\n",
      "iteration: 2 of max_iter: 50, perplexity: 1940.6496\n",
      "iteration: 3 of max_iter: 50, perplexity: 1903.4929\n",
      "iteration: 4 of max_iter: 50, perplexity: 1888.4079\n",
      "iteration: 5 of max_iter: 50, perplexity: 1881.9424\n",
      "iteration: 6 of max_iter: 50, perplexity: 1878.3228\n",
      "iteration: 7 of max_iter: 50, perplexity: 1876.5037\n",
      "iteration: 8 of max_iter: 50, perplexity: 1875.6214\n",
      "iteration: 9 of max_iter: 50, perplexity: 1874.6821\n",
      "iteration: 10 of max_iter: 50, perplexity: 1874.0066\n",
      "iteration: 11 of max_iter: 50, perplexity: 1874.2520\n",
      "iteration: 12 of max_iter: 50, perplexity: 1874.2706\n",
      "done in 323.837s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 1930.4221\n",
      "iteration: 2 of max_iter: 50, perplexity: 1830.0795\n",
      "iteration: 3 of max_iter: 50, perplexity: 1791.2598\n",
      "iteration: 4 of max_iter: 50, perplexity: 1775.2134\n",
      "iteration: 5 of max_iter: 50, perplexity: 1767.5105\n",
      "iteration: 6 of max_iter: 50, perplexity: 1763.1951\n",
      "iteration: 7 of max_iter: 50, perplexity: 1760.4235\n",
      "iteration: 8 of max_iter: 50, perplexity: 1758.9296\n",
      "iteration: 9 of max_iter: 50, perplexity: 1757.9380\n",
      "iteration: 10 of max_iter: 50, perplexity: 1757.1538\n",
      "iteration: 11 of max_iter: 50, perplexity: 1756.4201\n",
      "iteration: 12 of max_iter: 50, perplexity: 1755.8014\n",
      "iteration: 13 of max_iter: 50, perplexity: 1755.3078\n",
      "iteration: 14 of max_iter: 50, perplexity: 1755.0443\n",
      "iteration: 15 of max_iter: 50, perplexity: 1754.8396\n",
      "iteration: 16 of max_iter: 50, perplexity: 1754.8888\n",
      "done in 423.632s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 1805.3571\n",
      "iteration: 2 of max_iter: 50, perplexity: 1703.5257\n",
      "iteration: 3 of max_iter: 50, perplexity: 1660.7245\n",
      "iteration: 4 of max_iter: 50, perplexity: 1641.0516\n",
      "iteration: 5 of max_iter: 50, perplexity: 1630.3901\n",
      "iteration: 6 of max_iter: 50, perplexity: 1623.9012\n",
      "iteration: 7 of max_iter: 50, perplexity: 1619.5446\n",
      "iteration: 8 of max_iter: 50, perplexity: 1616.6160\n",
      "iteration: 9 of max_iter: 50, perplexity: 1614.2612\n",
      "iteration: 10 of max_iter: 50, perplexity: 1612.7138\n",
      "iteration: 11 of max_iter: 50, perplexity: 1611.3170\n",
      "iteration: 12 of max_iter: 50, perplexity: 1610.2826\n",
      "iteration: 13 of max_iter: 50, perplexity: 1609.4341\n",
      "iteration: 14 of max_iter: 50, perplexity: 1608.7115\n",
      "iteration: 15 of max_iter: 50, perplexity: 1608.0035\n",
      "iteration: 16 of max_iter: 50, perplexity: 1607.4812\n",
      "iteration: 17 of max_iter: 50, perplexity: 1607.0695\n",
      "iteration: 18 of max_iter: 50, perplexity: 1606.6906\n",
      "iteration: 19 of max_iter: 50, perplexity: 1606.3922\n",
      "iteration: 20 of max_iter: 50, perplexity: 1606.0056\n",
      "iteration: 21 of max_iter: 50, perplexity: 1605.7589\n",
      "iteration: 22 of max_iter: 50, perplexity: 1605.5424\n",
      "iteration: 23 of max_iter: 50, perplexity: 1605.3723\n",
      "iteration: 24 of max_iter: 50, perplexity: 1605.1584\n",
      "iteration: 25 of max_iter: 50, perplexity: 1604.9270\n",
      "iteration: 26 of max_iter: 50, perplexity: 1604.7858\n",
      "iteration: 27 of max_iter: 50, perplexity: 1604.6163\n",
      "iteration: 28 of max_iter: 50, perplexity: 1604.5185\n",
      "done in 714.517s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 1766.5153\n",
      "iteration: 2 of max_iter: 50, perplexity: 1660.3402\n",
      "iteration: 3 of max_iter: 50, perplexity: 1615.5741\n",
      "iteration: 4 of max_iter: 50, perplexity: 1594.5631\n",
      "iteration: 5 of max_iter: 50, perplexity: 1583.3523\n",
      "iteration: 6 of max_iter: 50, perplexity: 1576.3107\n",
      "iteration: 7 of max_iter: 50, perplexity: 1571.5256\n",
      "iteration: 8 of max_iter: 50, perplexity: 1568.0909\n",
      "iteration: 9 of max_iter: 50, perplexity: 1565.5409\n",
      "iteration: 10 of max_iter: 50, perplexity: 1563.6137\n",
      "iteration: 11 of max_iter: 50, perplexity: 1562.0730\n",
      "iteration: 12 of max_iter: 50, perplexity: 1560.6759\n",
      "iteration: 13 of max_iter: 50, perplexity: 1559.6072\n",
      "iteration: 14 of max_iter: 50, perplexity: 1558.6951\n",
      "iteration: 15 of max_iter: 50, perplexity: 1557.8623\n",
      "iteration: 16 of max_iter: 50, perplexity: 1557.1510\n",
      "iteration: 17 of max_iter: 50, perplexity: 1556.5953\n",
      "iteration: 18 of max_iter: 50, perplexity: 1556.0114\n",
      "iteration: 19 of max_iter: 50, perplexity: 1555.4925\n",
      "iteration: 20 of max_iter: 50, perplexity: 1555.0979\n",
      "iteration: 21 of max_iter: 50, perplexity: 1554.8164\n",
      "iteration: 22 of max_iter: 50, perplexity: 1554.5406\n",
      "iteration: 23 of max_iter: 50, perplexity: 1554.2831\n",
      "iteration: 24 of max_iter: 50, perplexity: 1554.0475\n",
      "iteration: 25 of max_iter: 50, perplexity: 1553.8464\n",
      "iteration: 26 of max_iter: 50, perplexity: 1553.6107\n",
      "iteration: 27 of max_iter: 50, perplexity: 1553.4465\n",
      "iteration: 28 of max_iter: 50, perplexity: 1553.3042\n",
      "iteration: 29 of max_iter: 50, perplexity: 1553.1738\n",
      "iteration: 30 of max_iter: 50, perplexity: 1553.0111\n",
      "iteration: 31 of max_iter: 50, perplexity: 1552.8471\n",
      "iteration: 32 of max_iter: 50, perplexity: 1552.6998\n",
      "iteration: 33 of max_iter: 50, perplexity: 1552.6009\n",
      "done in 833.134s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 1751.6624\n",
      "iteration: 2 of max_iter: 50, perplexity: 1634.6572\n",
      "iteration: 3 of max_iter: 50, perplexity: 1583.0651\n",
      "iteration: 4 of max_iter: 50, perplexity: 1557.5610\n",
      "iteration: 5 of max_iter: 50, perplexity: 1542.9592\n",
      "iteration: 6 of max_iter: 50, perplexity: 1533.3868\n",
      "iteration: 7 of max_iter: 50, perplexity: 1526.6981\n",
      "iteration: 8 of max_iter: 50, perplexity: 1521.7577\n",
      "iteration: 9 of max_iter: 50, perplexity: 1518.0106\n",
      "iteration: 10 of max_iter: 50, perplexity: 1515.0615\n",
      "iteration: 11 of max_iter: 50, perplexity: 1512.6751\n",
      "iteration: 12 of max_iter: 50, perplexity: 1510.7634\n",
      "iteration: 13 of max_iter: 50, perplexity: 1509.1779\n",
      "iteration: 14 of max_iter: 50, perplexity: 1507.8320\n",
      "iteration: 15 of max_iter: 50, perplexity: 1506.6768\n",
      "iteration: 16 of max_iter: 50, perplexity: 1505.7093\n",
      "iteration: 17 of max_iter: 50, perplexity: 1504.8546\n",
      "iteration: 18 of max_iter: 50, perplexity: 1504.0871\n",
      "iteration: 19 of max_iter: 50, perplexity: 1503.4199\n",
      "iteration: 20 of max_iter: 50, perplexity: 1502.8137\n",
      "iteration: 21 of max_iter: 50, perplexity: 1502.2714\n",
      "iteration: 22 of max_iter: 50, perplexity: 1501.8027\n",
      "iteration: 23 of max_iter: 50, perplexity: 1501.3710\n",
      "iteration: 24 of max_iter: 50, perplexity: 1500.9904\n",
      "iteration: 25 of max_iter: 50, perplexity: 1500.6354\n",
      "iteration: 26 of max_iter: 50, perplexity: 1500.3033\n",
      "iteration: 27 of max_iter: 50, perplexity: 1499.9825\n",
      "iteration: 28 of max_iter: 50, perplexity: 1499.7015\n",
      "iteration: 29 of max_iter: 50, perplexity: 1499.4187\n",
      "iteration: 30 of max_iter: 50, perplexity: 1499.1664\n",
      "iteration: 31 of max_iter: 50, perplexity: 1498.9250\n",
      "iteration: 32 of max_iter: 50, perplexity: 1498.7098\n",
      "iteration: 33 of max_iter: 50, perplexity: 1498.5064\n",
      "iteration: 38 of max_iter: 50, perplexity: 1497.6395\n",
      "iteration: 39 of max_iter: 50, perplexity: 1497.4874\n",
      "iteration: 40 of max_iter: 50, perplexity: 1497.3456\n",
      "iteration: 41 of max_iter: 50, perplexity: 1497.2108\n",
      "iteration: 42 of max_iter: 50, perplexity: 1497.0813\n",
      "iteration: 43 of max_iter: 50, perplexity: 1496.9572\n",
      "iteration: 44 of max_iter: 50, perplexity: 1496.8498\n",
      "iteration: 45 of max_iter: 50, perplexity: 1496.7500\n",
      "done in 1165.384s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 1796.5939\n",
      "iteration: 2 of max_iter: 50, perplexity: 1672.7142\n",
      "iteration: 3 of max_iter: 50, perplexity: 1613.5327\n",
      "iteration: 4 of max_iter: 50, perplexity: 1581.4879\n",
      "iteration: 5 of max_iter: 50, perplexity: 1561.8712\n",
      "iteration: 6 of max_iter: 50, perplexity: 1548.6341\n",
      "iteration: 7 of max_iter: 50, perplexity: 1539.2191\n",
      "iteration: 8 of max_iter: 50, perplexity: 1532.1903\n",
      "iteration: 9 of max_iter: 50, perplexity: 1526.6758\n",
      "iteration: 10 of max_iter: 50, perplexity: 1522.2225\n",
      "iteration: 11 of max_iter: 50, perplexity: 1518.5960\n",
      "iteration: 12 of max_iter: 50, perplexity: 1515.5813\n",
      "iteration: 13 of max_iter: 50, perplexity: 1513.0329\n",
      "iteration: 14 of max_iter: 50, perplexity: 1510.8438\n",
      "iteration: 15 of max_iter: 50, perplexity: 1508.9193\n",
      "iteration: 16 of max_iter: 50, perplexity: 1507.2316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 17 of max_iter: 50, perplexity: 1505.7361\n",
      "iteration: 18 of max_iter: 50, perplexity: 1504.3916\n",
      "iteration: 19 of max_iter: 50, perplexity: 1503.1799\n",
      "iteration: 20 of max_iter: 50, perplexity: 1502.0819\n",
      "iteration: 21 of max_iter: 50, perplexity: 1501.0794\n",
      "iteration: 22 of max_iter: 50, perplexity: 1500.1605\n",
      "iteration: 23 of max_iter: 50, perplexity: 1499.3110\n",
      "iteration: 24 of max_iter: 50, perplexity: 1498.5255\n",
      "iteration: 25 of max_iter: 50, perplexity: 1497.7963\n",
      "iteration: 26 of max_iter: 50, perplexity: 1497.1180\n",
      "iteration: 27 of max_iter: 50, perplexity: 1496.4793\n",
      "iteration: 28 of max_iter: 50, perplexity: 1495.8781\n",
      "iteration: 29 of max_iter: 50, perplexity: 1495.3102\n",
      "iteration: 30 of max_iter: 50, perplexity: 1494.7719\n",
      "iteration: 31 of max_iter: 50, perplexity: 1494.2613\n",
      "iteration: 32 of max_iter: 50, perplexity: 1493.7799\n",
      "iteration: 33 of max_iter: 50, perplexity: 1493.3238\n",
      "iteration: 34 of max_iter: 50, perplexity: 1492.8902\n",
      "iteration: 35 of max_iter: 50, perplexity: 1492.4800\n",
      "iteration: 36 of max_iter: 50, perplexity: 1492.0935\n",
      "iteration: 37 of max_iter: 50, perplexity: 1491.7253\n",
      "iteration: 38 of max_iter: 50, perplexity: 1491.3755\n",
      "iteration: 39 of max_iter: 50, perplexity: 1491.0459\n",
      "iteration: 40 of max_iter: 50, perplexity: 1490.7337\n",
      "iteration: 41 of max_iter: 50, perplexity: 1490.4379\n",
      "iteration: 47 of max_iter: 50, perplexity: 1488.9523\n",
      "iteration: 48 of max_iter: 50, perplexity: 1488.7448\n",
      "iteration: 49 of max_iter: 50, perplexity: 1488.5481\n",
      "iteration: 50 of max_iter: 50, perplexity: 1488.3603\n",
      "done in 1380.513s.\n"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../experiment/0714expt/mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# for batch_size,n_components, topic_word_prior, doc_topic_prior in itertools.product([1500] ,[6, 8, 10,12,15,18,22,25,30,35,40], # 6,8 のあと落ちた、、\n",
    "for batch_size,n_components, topic_word_prior, doc_topic_prior in itertools.product([1500] ,[18,22,25,30,35,40], # 6,8 のあと落ちた、、\n",
    "                                                                                    [0.005,0.01, 0.03,0.05, 0.15, 0.3, 0.4,  0.6, 0.8],[0.005,0.01, 0.03,0.05, 0.15, 0.3, 0.4,  0.6, 0.8] ):\n",
    "    now_ = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')\n",
    "    \n",
    "    lda_main(now_, n_components=n_components, topic_word_prior=topic_word_prior, doc_topic_prior = doc_topic_prior, batch_size=batch_size, max_iter=50)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pez5Ak2-cUjS"
   },
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_HRjNZJcbNr"
   },
   "source": [
    "### プロフィールを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prof (topic_idx, top_n = 100):\n",
    "    \n",
    "    topics = lda.transform(X)\n",
    "    prof_idx_list = topics[:, topic_idx].argsort()[:-top_n - 1:-1]\n",
    "    return [docs[d] for d in prof_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_top_prof(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nownow_file = (datetime.datetime.now() + datetime.timedelta(hours=9) ).strftime('%m%d_%H%M')+\"topic_modeling.ipynb\"\n",
    "\n",
    "!cp ./topic_modeling.ipynb ./jupyter_backup_for_param/$nownow_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')+\"_output.txt\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyVTNTZxoqrFDwo2+taBP8",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "12MxXsGfePft6pAHKkHrg23Yb7t_RSKiN",
   "name": "11_topic_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
