{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/11_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 標準使用ライブラリー\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from icecream import ic\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "\n",
    "\n",
    "# 追記\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "# debug\n",
    "#%pdb on\n",
    "\n",
    "import pixiedust #%pixie_debugger\n",
    "\n",
    "# tfがエラーはかないため\n",
    "# tfがエラーはかないため\n",
    "#import tensorflow as tf\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "df = pd.read_csv(\"./data/result0605.csv\", engine='python')\n",
    "\n",
    "type(df[\"description\"])\n",
    "docs = df[\"description\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RhY0Iq0ycIv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93794\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM4WRXtQgL9G"
   },
   "source": [
    "### Neologdを使ってtokenizeする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1LVnUt6FgYbW"
   },
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import MeCab\n",
    "\n",
    "def make_neologd_tagger():\n",
    "    cmd='echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "    path_neologd = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                               shell=True).communicate()[0]).decode('utf-8')\n",
    "    m=MeCab.Tagger(\"-Ochasen -d \"+str(path_neologd))\n",
    "    return (m)\n",
    "\n",
    "\n",
    "def neolog_prep_text( text, m):\n",
    "    return_words = []\n",
    "\n",
    "    \n",
    "    splited_text = (re.split('[\\t,]', line) for line in m.parse(text).split('\\n'))\n",
    "    for tmp_word in splited_text :\n",
    "        if (tmp_word[0] in ('EOS', '', 't', 'ー') ):\n",
    "           continue \n",
    "        if not re.match( '名詞' ,tmp_word[3]  ) or tmp_word[0] in emoji.UNICODE_EMOJI[\"en\"]:\n",
    "            continue\n",
    "        else:\n",
    "            return_words.append(tmp_word[0])\n",
    "\n",
    "    return return_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPvZBojxgYcW"
   },
   "source": [
    "* tokenizationの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1iICBt6EgqC-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93794/93794 [01:11<00:00, 1317.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "m = make_neologd_tagger()\n",
    "\n",
    "new_docs = list()\n",
    "for doc in tqdm(docs):\n",
    "  if str(doc) == \"nan\":\n",
    "    continue\n",
    "  tmp_words =  neolog_prep_text(str(doc), m)\n",
    "  new_docs.append( tmp_words )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBDnVSoZgaMJ"
   },
   "source": [
    "* tokenizationの結果を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p6MVn-YGhBst"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['過去', 'ジャパリカート', '動画', 'TSUMURI', 'KART', 'VRChat', 'ワリスノ', 'MK', '8', 'DX', '一位', 'りし', 'た人', '社会', '出て', '配信', 'https', 'co', 'FJoitl', '8', 'JHE', 'ヘッダ', '猫', '飼い主', 'smmmmm']\n"
     ]
    }
   ],
   "source": [
    "print(new_docs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7lABQFJgdBj"
   },
   "source": [
    "* 各文書を長い文字列で表しなおす（CountVectorizerを後で使うため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mnNRbzLzi6vh"
   },
   "outputs": [],
   "source": [
    "corpus = [' '.join(doc) for doc in new_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nu0eT7OXCzD"
   },
   "source": [
    "## 11-02 データ行列の作成\n",
    "* LDAの場合、単に単語の出現頻度を重みとして各文書をベクトル化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo8oPlaMCzoM"
   },
   "source": [
    "### sklearnのCountVectorizerで疎行列化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pyLBUfSXTUL"
   },
   "source": [
    "* 全文書の半分より多い文書に現れる単語は、高頻度語とみなして削除する。\n",
    "* 30件未満の文書にしか現れない単語は、低頻度語とみなして削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "def download_stopwords(path):\n",
    "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "    if os.path.exists(path):\n",
    "        print('File already exists.')\n",
    "    else:\n",
    "        print('Downloading...')\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "\n",
    "def create_stopwords(file_path):\n",
    "    stop_words = []\n",
    "    for w in open(path, \"r\"):\n",
    "        w = w.replace('\\n','')\n",
    "        if len(w) > 0:\n",
    "          stop_words.append(w)\n",
    "    return stop_words    \n",
    "\n",
    "path = \"stop_words.txt\"\n",
    "download_stopwords(path)\n",
    "stop_words = create_stopwords(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zrj0mmMrCzNI"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MIN_DF = 30\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df= MIN_DF, stop_words=stop_words)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XoE0bBHJEFEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2wVko9vXnlq"
   },
   "source": [
    "* 文書数と語彙サイズを変数にセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kAd821DGFXXp"
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8puM9OXrNk"
   },
   "source": [
    "### TF-IDFで各文書における単語の重みを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ywX2HtW-Elar"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "Xtfidf = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hdac5_tSE4YC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4227)\t0.603862693464167\n",
      "  (0, 4148)\t0.4706802938434637\n",
      "  (0, 3490)\t0.1847556021921528\n",
      "  (0, 3414)\t0.33418592758883475\n",
      "  (0, 3402)\t0.3582132311126842\n",
      "  (0, 1747)\t0.24047370992039763\n",
      "  (0, 1109)\t0.28609564411708244\n"
     ]
    }
   ],
   "source": [
    "print(Xtfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CY0mRZonFEJF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88481, 5100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXUdhYDMYuDO"
   },
   "source": [
    "### LDAのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5L34qQ1iFncJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import datetime\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPHsNjupYp7w"
   },
   "source": [
    "### トピックの重要語を取り出す関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rpoC-pofHMEO"
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words=30):\n",
    "  top_features = list()\n",
    "  weights = list()\n",
    "  for topic_idx, topic in enumerate(model.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features.append([feature_names[i] for i in top_features_ind])\n",
    "    weights.append(topic[top_features_ind])\n",
    "  return top_features, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EHfuc6RZgPh"
   },
   "source": [
    "# LDAでトピック抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSAEThMwZjhb"
   },
   "source": [
    "### LDAによるトピック抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NFf5jcX5b45d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_word_cloud(n_components, lda):\n",
    "    # matplotlib and seaborn for plotting\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    plt.style.use('dark_background')\n",
    "    top_words, weights = get_top_words(lda, vectorizer.get_feature_names())\n",
    "    topic_words = [dict(zip(top_words[i], weights[i])) for i in range(n_components)]\n",
    "    FONT_PATH = \"/usr/share/fonts/opentype/ipaexfont-mincho/ipaexm.ttf\"\n",
    "    cloud = WordCloud(stopwords=STOPWORDS,\n",
    "                  font_path=FONT_PATH,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=100,\n",
    "                  colormap='tab10'\n",
    "                  )\n",
    "\n",
    "    tate = math.ceil(len(topic_words) / 2)\n",
    "    fig, axes = plt.subplots(tate, 2, figsize=(32, 50), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "        if i >= len(topic_words):\n",
    "            break\n",
    "\n",
    "        fig.add_subplot(ax)\n",
    "        cloud.generate_from_frequencies(topic_words[i], max_font_size=500)\n",
    "        plt.gca().imshow(cloud)\n",
    "        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        plt.gca().axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    pdf = PdfPages( \n",
    "        (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M') + 'topic.pdf')\n",
    "\n",
    "\n",
    "    fignums = plt.get_fignums()\n",
    "    for fignum in fignums:\n",
    "        plt.figure(fignum)\n",
    "        pdf.savefig()\n",
    "\n",
    "    pdf.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KuuSTK6fZfu7"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "def lda_main (batch_size ,n_components, topic_word_prior,doc_topic_prior  ,max_iter=30):\n",
    "\n",
    "    folder_name = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')\n",
    "\n",
    "    # フォルダを作成\n",
    "    os.mkdir(\"./0701expt/\"+folder_name)\n",
    "    os.chdir(\"./0701expt/\"+folder_name)\n",
    "\n",
    "    # logging\n",
    "    logger = logging.getLogger()\n",
    "    fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.setLevel(logging.WARNING)\n",
    "    \n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                    max_iter=max_iter,\n",
    "                                    topic_word_prior=topic_word_prior, # トピック数の逆数が目安の0.01,0.02,0.05,0.1などなど試す\n",
    "                                    doc_topic_prior =  doc_topic_prior, \n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50,\n",
    "                                    batch_size= batch_size,# 多くする\n",
    "                                    learning_decay = 1.3,\n",
    "                                    mean_change_tol=1e-4,\n",
    "                                    random_state=1,\n",
    "                                    evaluate_every=1,\n",
    "                                    verbose=1)\n",
    "    print((f\"Fitting LDA models with tf features, \"\n",
    "    f\"n_samples={n_samples} and n_features={n_features}\"))\n",
    "    t0 = time()\n",
    "    lda.fit(X)\n",
    "    print(f\"done in {time() - t0:0.3f}s.\")\n",
    "    # パラメータの比較はperplexity\n",
    "    # ハイパーパラメータ調整を頑張る！（やってられない！といわない！！）\n",
    "    \n",
    "    \n",
    "    coherance = metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=20, # これはデフォルトが20\n",
    "                        topic_word_distrib=lda.components_, \n",
    "                        dtm=Xtfidf,  # tfidfの結果\n",
    "                        vocab=np.array([x for x in vectorizer.vocabulary_.keys()]), \n",
    "                        texts=new_docs)\n",
    "    \n",
    "    \n",
    "    results = {\n",
    "            \"perplexity\" : lda.perplexity(X) ,\n",
    "            \"coherance\": coherance,\n",
    "        }\n",
    "\n",
    "    logger.warning('MIN_DF:{0}'.format(MIN_DF) )\n",
    "    logger.warning('params:batch_size:{0}'.format(batch_size)) \n",
    "    logger.warning('params:n_components:{0}'.format(n_components)) \n",
    "    logger.warning('params:topic_word_prior:{0}'.format(topic_word_prior)) \n",
    "    logger.warning('params:doc_topic_prior:{0}'.format(doc_topic_prior)) \n",
    "    logger.warning('params:max_iter:{0}'.format(max_iter)) \n",
    "    logger.warning('done n_iter:{0}'.format(lda.n_iter_)) \n",
    "    logger.warning('perplexity:{0}'.format(results[\"perplexity\"])) \n",
    "    logger.warning('coherance:{0}'.format(results[\"coherance\"]) )\n",
    "    logger.warning('check all params:{0}'.format(lda.get_params() )) \n",
    "    make_word_cloud(n_components, lda)\n",
    "    # pickle\n",
    "    file_name = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M') + '_lda.pickle'\n",
    "    with open(file_name, mode=\"wb\") as f:\n",
    "        pickle.dump(lda, f)\n",
    "    \n",
    "#     breakpoint()\n",
    "    \n",
    "    os.chdir(\"../../\")\n",
    "    return(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWd3DND8aAVz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2025.4110\n",
      "iteration: 2 of max_iter: 50, perplexity: 1935.0141\n",
      "iteration: 3 of max_iter: 50, perplexity: 1898.8129\n",
      "iteration: 4 of max_iter: 50, perplexity: 1879.3315\n",
      "iteration: 5 of max_iter: 50, perplexity: 1866.5861\n",
      "iteration: 6 of max_iter: 50, perplexity: 1857.4725\n",
      "iteration: 7 of max_iter: 50, perplexity: 1850.4977\n",
      "iteration: 8 of max_iter: 50, perplexity: 1845.1638\n",
      "iteration: 9 of max_iter: 50, perplexity: 1840.9879\n",
      "iteration: 10 of max_iter: 50, perplexity: 1837.2042\n",
      "iteration: 11 of max_iter: 50, perplexity: 1833.9776\n",
      "iteration: 12 of max_iter: 50, perplexity: 1831.3422\n",
      "iteration: 13 of max_iter: 50, perplexity: 1829.0486\n",
      "iteration: 14 of max_iter: 50, perplexity: 1826.7040\n",
      "iteration: 15 of max_iter: 50, perplexity: 1824.9746\n",
      "iteration: 16 of max_iter: 50, perplexity: 1823.4095\n",
      "iteration: 17 of max_iter: 50, perplexity: 1822.0773\n",
      "iteration: 18 of max_iter: 50, perplexity: 1820.7940\n",
      "iteration: 19 of max_iter: 50, perplexity: 1819.3012\n",
      "iteration: 20 of max_iter: 50, perplexity: 1817.9831\n",
      "iteration: 21 of max_iter: 50, perplexity: 1816.7280\n",
      "iteration: 22 of max_iter: 50, perplexity: 1815.5504\n",
      "iteration: 23 of max_iter: 50, perplexity: 1814.4005\n",
      "iteration: 24 of max_iter: 50, perplexity: 1813.6527\n",
      "iteration: 25 of max_iter: 50, perplexity: 1812.7850\n",
      "iteration: 26 of max_iter: 50, perplexity: 1812.1759\n",
      "iteration: 27 of max_iter: 50, perplexity: 1811.4042\n",
      "iteration: 28 of max_iter: 50, perplexity: 1810.7073\n",
      "iteration: 29 of max_iter: 50, perplexity: 1809.9316\n",
      "iteration: 34 of max_iter: 50, perplexity: 1806.8410\n",
      "iteration: 35 of max_iter: 50, perplexity: 1806.3618\n",
      "iteration: 36 of max_iter: 50, perplexity: 1805.8500\n",
      "iteration: 37 of max_iter: 50, perplexity: 1805.4585\n",
      "iteration: 38 of max_iter: 50, perplexity: 1804.9948\n",
      "iteration: 39 of max_iter: 50, perplexity: 1804.6475\n",
      "iteration: 40 of max_iter: 50, perplexity: 1804.2322\n",
      "iteration: 41 of max_iter: 50, perplexity: 1803.8227\n",
      "iteration: 42 of max_iter: 50, perplexity: 1803.4886\n",
      "iteration: 43 of max_iter: 50, perplexity: 1803.0596\n",
      "iteration: 44 of max_iter: 50, perplexity: 1802.6810\n",
      "iteration: 45 of max_iter: 50, perplexity: 1802.3121\n",
      "iteration: 46 of max_iter: 50, perplexity: 1801.9739\n",
      "iteration: 47 of max_iter: 50, perplexity: 1801.6962\n",
      "iteration: 48 of max_iter: 50, perplexity: 1801.3835\n",
      "iteration: 49 of max_iter: 50, perplexity: 1801.0490\n",
      "iteration: 50 of max_iter: 50, perplexity: 1800.8585\n",
      "done in 1604.309s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2023.1803\n",
      "iteration: 2 of max_iter: 50, perplexity: 1920.9562\n",
      "iteration: 3 of max_iter: 50, perplexity: 1879.6632\n",
      "iteration: 4 of max_iter: 50, perplexity: 1855.9111\n",
      "iteration: 5 of max_iter: 50, perplexity: 1840.7664\n",
      "iteration: 6 of max_iter: 50, perplexity: 1829.7005\n",
      "iteration: 7 of max_iter: 50, perplexity: 1821.1115\n",
      "iteration: 8 of max_iter: 50, perplexity: 1814.5331\n",
      "iteration: 9 of max_iter: 50, perplexity: 1809.0892\n",
      "iteration: 10 of max_iter: 50, perplexity: 1804.4267\n",
      "iteration: 11 of max_iter: 50, perplexity: 1800.4881\n",
      "iteration: 12 of max_iter: 50, perplexity: 1797.1736\n",
      "iteration: 13 of max_iter: 50, perplexity: 1794.2463\n",
      "iteration: 14 of max_iter: 50, perplexity: 1791.5652\n",
      "iteration: 15 of max_iter: 50, perplexity: 1789.1855\n",
      "iteration: 16 of max_iter: 50, perplexity: 1787.0213\n",
      "iteration: 17 of max_iter: 50, perplexity: 1784.9554\n",
      "iteration: 18 of max_iter: 50, perplexity: 1783.1038\n",
      "iteration: 19 of max_iter: 50, perplexity: 1781.5076\n",
      "iteration: 20 of max_iter: 50, perplexity: 1780.0225\n",
      "iteration: 21 of max_iter: 50, perplexity: 1778.6088\n",
      "iteration: 22 of max_iter: 50, perplexity: 1777.2831\n",
      "iteration: 23 of max_iter: 50, perplexity: 1776.0893\n",
      "iteration: 24 of max_iter: 50, perplexity: 1775.0051\n",
      "iteration: 25 of max_iter: 50, perplexity: 1773.9141\n",
      "iteration: 26 of max_iter: 50, perplexity: 1772.8887\n",
      "iteration: 27 of max_iter: 50, perplexity: 1771.9448\n",
      "iteration: 28 of max_iter: 50, perplexity: 1771.0781\n",
      "iteration: 29 of max_iter: 50, perplexity: 1770.2262\n",
      "iteration: 30 of max_iter: 50, perplexity: 1769.4162\n",
      "iteration: 31 of max_iter: 50, perplexity: 1768.6192\n",
      "iteration: 32 of max_iter: 50, perplexity: 1767.8456\n",
      "iteration: 33 of max_iter: 50, perplexity: 1767.0985\n",
      "iteration: 34 of max_iter: 50, perplexity: 1766.4064\n",
      "iteration: 38 of max_iter: 50, perplexity: 1764.0555\n",
      "iteration: 39 of max_iter: 50, perplexity: 1763.4703\n",
      "iteration: 40 of max_iter: 50, perplexity: 1762.9253\n",
      "iteration: 41 of max_iter: 50, perplexity: 1762.3520\n",
      "iteration: 42 of max_iter: 50, perplexity: 1761.8699\n",
      "iteration: 43 of max_iter: 50, perplexity: 1761.4430\n",
      "iteration: 44 of max_iter: 50, perplexity: 1760.9701\n",
      "iteration: 45 of max_iter: 50, perplexity: 1760.5308\n",
      "iteration: 46 of max_iter: 50, perplexity: 1760.0651\n",
      "iteration: 47 of max_iter: 50, perplexity: 1759.6434\n",
      "iteration: 48 of max_iter: 50, perplexity: 1759.2636\n",
      "iteration: 49 of max_iter: 50, perplexity: 1758.8049\n",
      "iteration: 50 of max_iter: 50, perplexity: 1758.4399\n",
      "done in 1602.644s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2051.8238\n",
      "iteration: 2 of max_iter: 50, perplexity: 1944.0426\n",
      "iteration: 3 of max_iter: 50, perplexity: 1901.6499\n",
      "iteration: 4 of max_iter: 50, perplexity: 1877.5578\n",
      "iteration: 5 of max_iter: 50, perplexity: 1861.9762\n",
      "iteration: 6 of max_iter: 50, perplexity: 1850.6730\n",
      "iteration: 7 of max_iter: 50, perplexity: 1841.9837\n",
      "iteration: 8 of max_iter: 50, perplexity: 1835.0645\n",
      "iteration: 9 of max_iter: 50, perplexity: 1829.4029\n",
      "iteration: 10 of max_iter: 50, perplexity: 1824.6096\n",
      "iteration: 11 of max_iter: 50, perplexity: 1820.3533\n",
      "iteration: 12 of max_iter: 50, perplexity: 1816.8459\n",
      "iteration: 13 of max_iter: 50, perplexity: 1813.7558\n",
      "iteration: 14 of max_iter: 50, perplexity: 1810.8088\n",
      "iteration: 15 of max_iter: 50, perplexity: 1808.2616\n",
      "iteration: 16 of max_iter: 50, perplexity: 1806.0022\n",
      "iteration: 17 of max_iter: 50, perplexity: 1803.9004\n",
      "iteration: 18 of max_iter: 50, perplexity: 1801.9408\n",
      "iteration: 19 of max_iter: 50, perplexity: 1800.2321\n",
      "iteration: 20 of max_iter: 50, perplexity: 1798.6597\n",
      "iteration: 21 of max_iter: 50, perplexity: 1797.1830\n",
      "iteration: 22 of max_iter: 50, perplexity: 1795.7780\n",
      "iteration: 23 of max_iter: 50, perplexity: 1794.4380\n",
      "iteration: 24 of max_iter: 50, perplexity: 1793.2143\n",
      "iteration: 25 of max_iter: 50, perplexity: 1792.0546\n",
      "iteration: 26 of max_iter: 50, perplexity: 1790.9145\n",
      "iteration: 27 of max_iter: 50, perplexity: 1789.9144\n",
      "iteration: 28 of max_iter: 50, perplexity: 1788.9070\n",
      "iteration: 29 of max_iter: 50, perplexity: 1788.0360\n",
      "iteration: 30 of max_iter: 50, perplexity: 1787.1388\n",
      "iteration: 31 of max_iter: 50, perplexity: 1786.2942\n",
      "iteration: 32 of max_iter: 50, perplexity: 1785.5063\n",
      "iteration: 33 of max_iter: 50, perplexity: 1784.7443\n",
      "iteration: 34 of max_iter: 50, perplexity: 1784.0291\n",
      "iteration: 35 of max_iter: 50, perplexity: 1783.3283\n",
      "iteration: 36 of max_iter: 50, perplexity: 1782.6477\n",
      "iteration: 37 of max_iter: 50, perplexity: 1782.0273\n",
      "iteration: 38 of max_iter: 50, perplexity: 1781.4221\n",
      "iteration: 42 of max_iter: 50, perplexity: 1779.2021\n",
      "iteration: 43 of max_iter: 50, perplexity: 1778.6874\n",
      "iteration: 44 of max_iter: 50, perplexity: 1778.1711\n",
      "iteration: 45 of max_iter: 50, perplexity: 1777.7365\n",
      "iteration: 46 of max_iter: 50, perplexity: 1777.2749\n",
      "iteration: 47 of max_iter: 50, perplexity: 1776.8245\n",
      "iteration: 48 of max_iter: 50, perplexity: 1776.4240\n",
      "iteration: 49 of max_iter: 50, perplexity: 1776.0127\n",
      "iteration: 50 of max_iter: 50, perplexity: 1775.5983\n",
      "done in 1635.451s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2394.0212\n",
      "iteration: 2 of max_iter: 50, perplexity: 2258.3205\n",
      "iteration: 3 of max_iter: 50, perplexity: 2203.1749\n",
      "iteration: 4 of max_iter: 50, perplexity: 2171.5297\n",
      "iteration: 5 of max_iter: 50, perplexity: 2150.3992\n",
      "iteration: 6 of max_iter: 50, perplexity: 2134.9860\n",
      "iteration: 7 of max_iter: 50, perplexity: 2123.1560\n",
      "iteration: 8 of max_iter: 50, perplexity: 2113.6503\n",
      "iteration: 9 of max_iter: 50, perplexity: 2105.7759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10 of max_iter: 50, perplexity: 2099.1099\n",
      "iteration: 11 of max_iter: 50, perplexity: 2093.3779\n",
      "iteration: 12 of max_iter: 50, perplexity: 2088.4213\n",
      "iteration: 13 of max_iter: 50, perplexity: 2084.0145\n",
      "iteration: 14 of max_iter: 50, perplexity: 2080.0149\n",
      "iteration: 15 of max_iter: 50, perplexity: 2076.4252\n",
      "iteration: 16 of max_iter: 50, perplexity: 2073.1898\n",
      "iteration: 17 of max_iter: 50, perplexity: 2070.1861\n",
      "iteration: 18 of max_iter: 50, perplexity: 2067.4477\n",
      "iteration: 19 of max_iter: 50, perplexity: 2064.9524\n",
      "iteration: 20 of max_iter: 50, perplexity: 2062.6260\n",
      "iteration: 21 of max_iter: 50, perplexity: 2060.4238\n",
      "iteration: 22 of max_iter: 50, perplexity: 2058.3480\n",
      "iteration: 23 of max_iter: 50, perplexity: 2056.3906\n",
      "iteration: 24 of max_iter: 50, perplexity: 2054.5777\n",
      "iteration: 25 of max_iter: 50, perplexity: 2052.8420\n",
      "iteration: 26 of max_iter: 50, perplexity: 2051.2296\n",
      "iteration: 27 of max_iter: 50, perplexity: 2049.7263\n",
      "iteration: 28 of max_iter: 50, perplexity: 2048.2762\n",
      "iteration: 29 of max_iter: 50, perplexity: 2046.8692\n",
      "iteration: 30 of max_iter: 50, perplexity: 2045.5602\n",
      "iteration: 31 of max_iter: 50, perplexity: 2044.3094\n",
      "iteration: 32 of max_iter: 50, perplexity: 2043.0838\n",
      "iteration: 33 of max_iter: 50, perplexity: 2041.9027\n",
      "iteration: 34 of max_iter: 50, perplexity: 2040.7798\n",
      "iteration: 35 of max_iter: 50, perplexity: 2039.7146\n",
      "iteration: 36 of max_iter: 50, perplexity: 2038.6915\n",
      "iteration: 37 of max_iter: 50, perplexity: 2037.6895\n",
      "iteration: 38 of max_iter: 50, perplexity: 2036.7112\n",
      "iteration: 42 of max_iter: 50, perplexity: 2033.1535\n",
      "iteration: 43 of max_iter: 50, perplexity: 2032.3291\n",
      "iteration: 44 of max_iter: 50, perplexity: 2031.5151\n",
      "iteration: 45 of max_iter: 50, perplexity: 2030.7368\n",
      "iteration: 46 of max_iter: 50, perplexity: 2029.9853\n",
      "iteration: 47 of max_iter: 50, perplexity: 2029.2533\n",
      "iteration: 48 of max_iter: 50, perplexity: 2028.5326\n",
      "iteration: 49 of max_iter: 50, perplexity: 2027.8257\n",
      "iteration: 50 of max_iter: 50, perplexity: 2027.1461\n",
      "done in 1811.145s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2635.8650\n",
      "iteration: 2 of max_iter: 50, perplexity: 2452.3336\n",
      "iteration: 3 of max_iter: 50, perplexity: 2372.8821\n",
      "iteration: 4 of max_iter: 50, perplexity: 2326.9127\n",
      "iteration: 5 of max_iter: 50, perplexity: 2296.2867\n",
      "iteration: 6 of max_iter: 50, perplexity: 2274.0598\n",
      "iteration: 7 of max_iter: 50, perplexity: 2256.9935\n",
      "iteration: 8 of max_iter: 50, perplexity: 2243.3638\n",
      "iteration: 9 of max_iter: 50, perplexity: 2232.1547\n",
      "iteration: 10 of max_iter: 50, perplexity: 2222.7207\n",
      "iteration: 11 of max_iter: 50, perplexity: 2214.6316\n",
      "iteration: 12 of max_iter: 50, perplexity: 2207.6042\n",
      "iteration: 13 of max_iter: 50, perplexity: 2201.4234\n",
      "iteration: 14 of max_iter: 50, perplexity: 2195.9262\n",
      "iteration: 15 of max_iter: 50, perplexity: 2190.9921\n",
      "iteration: 16 of max_iter: 50, perplexity: 2186.5324\n",
      "iteration: 17 of max_iter: 50, perplexity: 2182.4744\n",
      "iteration: 18 of max_iter: 50, perplexity: 2178.7610\n",
      "iteration: 19 of max_iter: 50, perplexity: 2175.3380\n",
      "iteration: 20 of max_iter: 50, perplexity: 2172.1737\n",
      "iteration: 21 of max_iter: 50, perplexity: 2169.2361\n",
      "iteration: 22 of max_iter: 50, perplexity: 2166.4985\n",
      "iteration: 23 of max_iter: 50, perplexity: 2163.9406\n",
      "iteration: 24 of max_iter: 50, perplexity: 2161.5388\n",
      "iteration: 25 of max_iter: 50, perplexity: 2159.2814\n",
      "iteration: 26 of max_iter: 50, perplexity: 2157.1506\n",
      "iteration: 27 of max_iter: 50, perplexity: 2155.1401\n",
      "iteration: 28 of max_iter: 50, perplexity: 2153.2334\n",
      "iteration: 29 of max_iter: 50, perplexity: 2151.4262\n",
      "iteration: 30 of max_iter: 50, perplexity: 2149.7072\n",
      "iteration: 33 of max_iter: 50, perplexity: 2145.0146\n",
      "iteration: 34 of max_iter: 50, perplexity: 2143.5874\n",
      "iteration: 35 of max_iter: 50, perplexity: 2142.2187\n",
      "iteration: 36 of max_iter: 50, perplexity: 2140.9054\n",
      "iteration: 37 of max_iter: 50, perplexity: 2139.6425\n",
      "iteration: 38 of max_iter: 50, perplexity: 2138.4298\n",
      "iteration: 39 of max_iter: 50, perplexity: 2137.2639\n",
      "iteration: 40 of max_iter: 50, perplexity: 2136.1419\n",
      "iteration: 41 of max_iter: 50, perplexity: 2135.0571\n",
      "iteration: 42 of max_iter: 50, perplexity: 2134.0117\n",
      "iteration: 43 of max_iter: 50, perplexity: 2132.9998\n",
      "iteration: 44 of max_iter: 50, perplexity: 2132.0224\n",
      "iteration: 45 of max_iter: 50, perplexity: 2131.0742\n",
      "iteration: 46 of max_iter: 50, perplexity: 2130.1573\n",
      "iteration: 47 of max_iter: 50, perplexity: 2129.2685\n",
      "iteration: 48 of max_iter: 50, perplexity: 2128.4070\n",
      "iteration: 49 of max_iter: 50, perplexity: 2127.5691\n",
      "iteration: 50 of max_iter: 50, perplexity: 2126.7560\n",
      "done in 2193.697s.\n",
      "Fitting LDA models with tf features, n_samples=88481 and n_features=5100\n",
      "iteration: 1 of max_iter: 50, perplexity: 2746.6944\n",
      "iteration: 2 of max_iter: 50, perplexity: 2545.6642\n",
      "iteration: 3 of max_iter: 50, perplexity: 2460.0564\n",
      "iteration: 4 of max_iter: 50, perplexity: 2410.2363\n",
      "iteration: 5 of max_iter: 50, perplexity: 2376.7310\n",
      "iteration: 6 of max_iter: 50, perplexity: 2352.2315\n",
      "iteration: 7 of max_iter: 50, perplexity: 2333.3020\n",
      "iteration: 8 of max_iter: 50, perplexity: 2318.0931\n",
      "iteration: 9 of max_iter: 50, perplexity: 2305.5149\n",
      "iteration: 10 of max_iter: 50, perplexity: 2294.8818\n",
      "iteration: 11 of max_iter: 50, perplexity: 2285.7318\n",
      "iteration: 12 of max_iter: 50, perplexity: 2277.7466\n",
      "iteration: 13 of max_iter: 50, perplexity: 2270.6961\n",
      "iteration: 14 of max_iter: 50, perplexity: 2264.4093\n",
      "iteration: 15 of max_iter: 50, perplexity: 2258.7548\n",
      "iteration: 16 of max_iter: 50, perplexity: 2253.6321\n",
      "iteration: 17 of max_iter: 50, perplexity: 2248.9619\n",
      "iteration: 18 of max_iter: 50, perplexity: 2244.6802\n",
      "iteration: 19 of max_iter: 50, perplexity: 2240.7346\n",
      "iteration: 20 of max_iter: 50, perplexity: 2237.0823\n",
      "iteration: 21 of max_iter: 50, perplexity: 2233.6880\n",
      "iteration: 25 of max_iter: 50, perplexity: 2222.1615\n",
      "iteration: 26 of max_iter: 50, perplexity: 2219.6919\n",
      "iteration: 27 of max_iter: 50, perplexity: 2217.3564\n",
      "iteration: 28 of max_iter: 50, perplexity: 2215.1428\n",
      "iteration: 29 of max_iter: 50, perplexity: 2213.0409\n",
      "iteration: 30 of max_iter: 50, perplexity: 2211.0411\n",
      "iteration: 31 of max_iter: 50, perplexity: 2209.1353\n",
      "iteration: 32 of max_iter: 50, perplexity: 2207.3163\n",
      "iteration: 33 of max_iter: 50, perplexity: 2205.5772\n",
      "iteration: 34 of max_iter: 50, perplexity: 2203.9122\n",
      "iteration: 35 of max_iter: 50, perplexity: 2202.3162\n",
      "iteration: 36 of max_iter: 50, perplexity: 2200.7842\n",
      "iteration: 37 of max_iter: 50, perplexity: 2199.3121\n",
      "iteration: 38 of max_iter: 50, perplexity: 2197.8960\n",
      "iteration: 39 of max_iter: 50, perplexity: 2196.5321\n",
      "iteration: 40 of max_iter: 50, perplexity: 2195.2175\n",
      "iteration: 41 of max_iter: 50, perplexity: 2193.9491\n",
      "iteration: 42 of max_iter: 50, perplexity: 2192.7242\n",
      "iteration: 43 of max_iter: 50, perplexity: 2191.5403\n",
      "iteration: 44 of max_iter: 50, perplexity: 2190.3950\n",
      "iteration: 45 of max_iter: 50, perplexity: 2189.2862\n",
      "iteration: 46 of max_iter: 50, perplexity: 2188.2120\n",
      "iteration: 47 of max_iter: 50, perplexity: 2187.1706\n",
      "iteration: 48 of max_iter: 50, perplexity: 2186.1604\n"
     ]
    }
   ],
   "source": [
    "for batch_size,n_components, topic_word_prior, doc_topic_prior in itertools.product([1500,3000 ] ,[10,12,15,18,22,25,30,35,40], # 6,8 のあと落ちた、、\n",
    "                                                                                    [0.01, 0.03,0.05, 0.15, 0.3, 0.4, 0.5, 0.6, 0.8],[0.01, 0.03,0.05, 0.15, 0.3, 0.4, 0.5, 0.6, 0.8] ):\n",
    "\n",
    "    lda_main(n_components=n_components, topic_word_prior=topic_word_prior, doc_topic_prior = doc_topic_prior, batch_size=batch_size, max_iter=50)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pez5Ak2-cUjS"
   },
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_HRjNZJcbNr"
   },
   "source": [
    "### プロフィールを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_prof (topic_idx, top_n = 100):\n",
    "    \n",
    "    topics = lda.transform(X)\n",
    "    prof_idx_list = topics[:, topic_idx].argsort()[:-top_n - 1:-1]\n",
    "    return [docs[d] for d in prof_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_top_prof(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_top_prof(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nownow_file = (datetime.datetime.now() + datetime.timedelta(hours=9) ).strftime('%m%d_%H%M')+\"topic_modeling.ipynb\"\n",
    "\n",
    "!cp ./topic_modeling.ipynb ./jupyter_backup_for_param/$nownow_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = (datetime.datetime.now() + datetime.timedelta(hours=9) ) .strftime('%m%d_%H%M')+\"_output.txt\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyVTNTZxoqrFDwo2+taBP8",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "12MxXsGfePft6pAHKkHrg23Yb7t_RSKiN",
   "name": "11_topic_modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
